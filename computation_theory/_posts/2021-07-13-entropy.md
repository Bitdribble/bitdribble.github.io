---
layout: post
title: "Entropy"
author:
- Andrei Radulescu-Banu
---
References:
* C. Shannon: [A Mathematical Theory of Communication](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) (1948)
* E.T. Jaynes: [Probability Theory: The Logic of Science](https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712) (2003)
* O. Rioul: [This is IT: A Primer on Shannonâ€™s Entropy and Information](http://www.bourbaphy.fr/rioul.pdf), [video](https://www.youtube.com/watch?v=vinCEpee-tc)

Suppose we start with a discrete probability distribution $$(p_1, p_2, ..., p_n)$$, where $$0 \le p_k \le 1$$, and $$\Sum p_k = 1$$.
