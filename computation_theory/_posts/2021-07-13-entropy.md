---
layout: post
mathjax: true
title: "Entropy"
author:
- Andrei Radulescu-Banu
---
References:
* C. Shannon: [A Mathematical Theory of Communication](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) (1948)
* E.T. Jaynes: [Probability Theory: The Logic of Science](https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712) (2003)
* K. Mallick: [Thermodynamics and Information Theory](http://www.bourbaphy.fr/mallick.pdf), [video](https://www.youtube.com/watch?v=pXyONXaqqP8)
* O. Rioul: [This is IT: A Primer on Shannonâ€™s Entropy and Information](http://www.bourbaphy.fr/rioul.pdf), [video](https://www.youtube.com/watch?v=vinCEpee-tc)

Suppose we start with a discrete probability distribution $$(p_1, p_2, ..., p_n)$$, where $$0 \le p_k \le 1$$, and $$\sum p_k = 1$$. How would we define the entropy, or the amount of information $$H(p_1, ..., p_n)$$ given by the probability distribution?

We can reason by analogy with thermodynamics, see [Mallick]((http://www.bourbaphy.fr/mallick.pdf)), where _entropy of a microcanonical system enumerates the total number of microscopic states_, $$\Omega(E, V)$$, _and the Bolzmann formula states that entropy is_

$$
\begin{align*}
S = k_B log \Omega \,\,\, \mathrm{with} \,\,\, k_B \sim 1.3810^{-23}
\end{align*}
$$

