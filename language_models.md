---
layout: page
mathjax: true
title: Language Models
---

* [Docs](https://weaviate.io/developers/weaviate), [github](), [Slack](https://weaviate.slack.com)

#### Courses
* [Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/) (2023)
  * Charles Frye: [Learn to Spell: Prompt Engineering](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/prompt-engineering/)
    * D. Dohan et al: [The Language Model Cascades](https://arxiv.org/pdf/2207.10342.pdf) (2022)
    * The primary goal of prompting is subtractive; it focuses the mass of predictions to hone in on a specific world by conditioning the probabilistic model.
  * Josh Tobin:
    * [Prompt Engineering](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/prompt-engineering/)
    * [LLMOps](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/)
  * Harrison Chase: [Agents](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/chase-agents/)
* Maxime Labonne: [Large Language Model Course](https://github.com/mlabonne/llm-course), [blog](https://mlabonne.github.io/blog/)
  * Inspired by [DevOps-Roadmap](https://github.com/milanm/DevOps-Roadmap)
  * Based on [gists from younesbelkada](https://gist.github.com/younesbelkada)
* NYU CSCI 2590
  * Hyung Won Chung: [Instruction finetuning and RLHF lecture](https://www.youtube.com/watch?v=zjrM-MW-0y0&t=2186s) (2023)

#### Articles
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf), J. Devlin et al (2019)
* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html), A. Rush et al (2018)
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), A. Vaswani et al (2017)
* [Doing more with less: meta-reasoning and meta-learning in humans and machines](https://cocosci.princeton.edu/papers/doing-more-with-less.pdf) (2023)
* R. Bommasani et al: [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf) (2022)
* Doug Lenat: [Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc](https://arxiv.org/ftp/arxiv/papers/2308/2308.04445.pdf) (2023)
* X. Li et al: [Self-Alignment with Instruction Backtranslation](https://arxiv.org/pdf/2308.06259.pdf), Meta (2023)
  * AI Business: [Meet Humpback: Meta’s New AI Model That’s a Whale of an Upgrade of Llama](https://aibusiness.com/ml/meet-humpback-meta-s-new-ai-model-that-s-a-whale-of-an-upgrade-of-llama)
* Y. Wang et al: [SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf) (2023)
* H. Touvron et al: [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf) (2023)
* A.N. Lee et al: [Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://arxiv.org/pdf/2308.07317.pdf) (2023)
* Y. Perlitz et al: [Efficient Benchmarking (of Language Models)](https://arxiv.org/pdf/2308.11696.pdf) (2023). Refines HELM benchmark to rule out low quality results early on.
* B. Roziere et al: [Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950.pdf) (2023)
* N. Houlsby et al: [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf) (2023): use adapters for model tuning.
* X.L. Li, P. Liang: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf) (2021)
* B. Lester et al: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) (2021), a simplification of the prev article on prefix tuning
* [DeepSeek-V3 Technical Report](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) (2024)
* Cameron Wolfe: [Scaling Laws for LLMs: From GPT-3 to o3](https://cameronrwolfe.substack.com/p/scaling-laws-for-llms-from-gpt-3) (2024)
* Nathan Lambert: [OpenAI's o3: The grand finale of AI in 2024](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai) (2024)
* S. Welleck et al: [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/pdf/2406.16838) (2024)

#### Economic Impact:
* McKinsey
  * [Technology’s generational moment with generative AI: A CIO and CTO guide](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide) (2023)
  * [The economic potential of generative AI: The next productivity frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-AI-the-next-productivity-frontier#work-and-productivity) (2023)

#### Conferences
* [ICML '23](https://icml.cc/virtual/2023/events/workshop)
  * [Challenges of Deploying Generative AI](https://deployinggenerativeai.github.io/)
  * [Workshop on Theory-of-Mind](https://tomworkshop.github.io/)
  * [Interpretable Machine Learning in Healthcare](https://sites.google.com/view/imlh2023/home?authuser=1)

#### News
* Belle Lin, WSJ: [Companies Weigh Growing Power of Cloud Providers Amid AI Boom](https://www.wsj.com/articles/companies-weigh-growing-power-of-cloud-providers-amid-ai-boom-478c454a) (2023)

#### Language Models
* [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf), J. Hoffman et al (2022)
* [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf), A. Chowdhery et al (2022)
* [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/), G. Lample et al, Meta (2023)
* [Code Llama: Open Foundation Models for Code](https://scontent.fphl1-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=BnkB4kcpz5AAX9Oc-m-&_nc_ht=scontent.fphl1-1.fna&oh=00_AfA9H1CFM3LjWZ-32bnJtejUJghfnG016Di5hJHGZGnonQ&oe=64ECB20F) (2023)
* Eugene Yan:
  * Rishabh Agarwal et al: [Language Modeling Reading List (to Start Your Paper Club)](https://eugeneyan.com/writing/llm-reading-list/) (2024), Google Deep Mind
* Cameron Wolfe: [LLaMa-3 "kitchen-sink" approach](https://twitter.com/cwolferesearch/status/1782801731383943452), tweet (2024)
* Andrej Karpathy: [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU) (2024)
* Sebastian Raschka: [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch) (2024)
* Xu Owen He: [Mixture of A Million Experts](https://arxiv.org/pdf/2407.04153) (2024)

#### Prompt Engineering
* [Many-Shot In-Context Learning](https://arxiv.org/pdf/2404.11018)
* Cameron Wolfe: [Modern Advances in Prompt Engineering](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering) (2024)
* [promptingguide.ai](https://www.promptingguide.ai/)

#### Compound LLMs
* [DSPy](https://github.com/stanfordnlp/dspy)
* [DSPy docs](https://dspy-docs.vercel.app)
* Matt Yates, Sephora, [Prompt Engineering is Dead - Build LLM Applications with DSPy Framework](https://www.youtube.com/watch?v=D2HurSldDkE) (2024)
* L. Chen, B. Hanin et al: [Are More LM Calls All You Need? Towards the Scaling Properties of Compound AI Systems](https://arxiv.org/pdf/2403.02419) (2024)

#### RL for LLMs
* Cameron Wolfe: [Q-Learning for LLMs](https://twitter.com/cwolferesearch/status/1727727148859797600), twitter post (2024)

#### Explainability
* OpenAI: S. Bills et al: [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) (2023)

#### Evaluation
* Eugene Yan: [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/#evals-to-measure-performance) (2023)
* Lianmin Zheng et al: [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) (2023)
* Cameron Wolfe:
  * [LinkedIn post on Evaluating LLMs](https://www.linkedin.com/feed/update/urn:li:activity:7188220212880592896/) (2024)
  * [Using LLMs for Evaluation](https://cameronrwolfe.substack.com/p/llm-as-a-judge)
* Y. Chang et al: [A Survey on Evaluation of Large Language Models](https://arxiv.org/pdf/2307.03109) (2023)

#### Tech stack
*  Matt Bornstein, Rajko Radovanovic: [Emerging Architectures for LLM Applications](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/) (2023)
   * Andrei's [tweet](https://twitter.com/bitdribble/status/1672233355914428417)
* A16z starter kits
  * [A tool stack for building AI apps with JavaScript](https://a16z.com/2023/06/21/the-getting-started-with-ai-stack-for-javascript), [github](https://github.com/a16z-infra/ai-getting-started), [discored]](https://discord.com/invite/PQUmTBTGmT)
  * [A tool stack for building AI companions](https://github.com/a16z-infra/companion-app)
  * [A chatbot build on Meta’s Llama2 open source model](https://github.com/a16z-infra/llama2-chatbot)
* OpenAI
  * [GPT-3.5 Turbo fine-tuning and API updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates) (2023), [docs](https://platform.openai.com/docs/guides/fine-tuning)
  * [GPT best practices](https://platform.openai.com/docs/guides/gpt-best-practices)
* [LLama-Factory](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file)
  * Aishwarya Naresh Reganti [LinkedIn post](https://www.linkedin.com/posts/areganti_genai-llms-finetuning-activity-7189246779328974848-5M0s?utm_source=share&utm_medium=member_desktop)
* [LiteLLM](https://github.com/BerriAI/litellm)

#### Datasets
* [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)

#### Derivatives of LLaMA
* E. Hu et al: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685), 2021, [github](https://github.com/microsoft/LoRA)
  * LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture
  * Greatly reducing the number of trainable parameters for downstream tasks
  * LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers
  * The simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing `no inference latency` compared to a fully fine-tuned model, by construction.
  * Should work with dense layers
  * Benefits
    * The most significant benefit comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 if r << dmodel as we do not need to store the optimizer states for the frozen parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000× (from 350GB to 35MB)4. This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup during training on GPT-3 175B compared to full fine-tuning5 as we do not need to calculate the
gradient for the vast majority of the parameters    
* C. Wu et al: [PMC-LLaMA: Further Finetuning LLaMA on Medical Papers](https://arxiv.org/pdf/2304.14454.pdf) (2023)
* aituts.com: Yubin: [How to run Meta's LLaMA on your computer (Windows, Linux tutorial)](https://aituts.com/llama/) (Mar 2023)

#### Text Classification
* L. Tunstall, N. Reimers et al: [Efficient Few-Shot Learning Without Prompts](https://arxiv.org/pdf/2209.11055.pdf) (2022), [youtube](https://www.youtube.com/watch?v=6WBK7XSXJM8)
  * Phil Schmid: [Outperform OpenAI GPT-3 with SetFit for text-classification](https://www.philschmid.de/getting-started-setfit) (2022)
  * [SetFit: Efficient Few-Shot Learning Without Prompts](https://huggingface.co/blog/setfit)
  * Atharva Ingle, Weights and Biases: [SetFit: Efficient Few-Shot Learning Without Prompts](https://wandb.ai/gladiator/SetFit/reports/SetFit-Efficient-Few-Shot-Learning-Without-Prompts--VmlldzozMDUyMzk2)
  * Nils Rehmers, Cohere: [High quality text classification with few training examples with SetFit](https://www.youtube.com/watch?v=IHalt4Nbf_Q) (2023)

#### Named Entity Recognition (NER)
* [NER extraction using LangChain and LLMs codes explained](https://www.youtube.com/watch?v=OagbDJvywJI) (2023)
* Patrick Meyer: [Entity Recognition with LLM: A Complete Evaluation](https://pub.towardsai.net/entity-recognition-with-llm-a-complete-evaluation-e34eb1902149) (2023)
* [Entity Extraction: LLMs Versus Classical Neural Model + Live-Updating Knowledge Graph](https://blog.gdeltproject.org/entity-extraction-llms-versus-classical-neural-model-live-updating-knowledge-graph/) (2023)
* Google [Natural Language API demo](https://cloud.google.com/natural-language?hl=en)
* [Named Entity Recognition With HuggingFace Using PyTorch and W&B](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Named-Entity-Recognition-With-HuggingFace-Using-PyTorch-and-W-B--Vmlldzo0NDgzODA2) (2023)
* Tirendaz AI: [Named Entity Recognition with Hugging Face 🤗 NLP Tutorial For Beginners](https://www.youtube.com/watch?v=r-yR8-7dlvQ) (2023)
* [SpaCy](https://spacy.io/usage/spacy-101)

#### Code Generation
* Ziyin Zhang et al: [Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code](https://arxiv.org/abs/2311.07989) (2023), [github](https://github.com/codefuse-ai/Awesome-Code-LLM)


#### Courses
* A. Karpathy: [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY) (2023)
  * Github: [nanoGPT](https://github.com/karpathy/nanoGPT), Andrei's [fork](https://github.com/andrei-radulescu-banu/nanoGPT)
  * Notebook: [gpt_dev.ipynb](https://github.com/andrei-radulescu-banu/nanoGPT/blob/master/gpt_dev.ipynb)
  * Tokenizers: [sentencepiece](https://github.com/google/sentencepiece), [tiktoken](https://github.com/openai/tiktoken)


#### Posts
* Veysel Kocaman: [Introduction to Spark NLP: Foundations and Basic Components](https://towardsdatascience.com/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59) (2019)
* [Lilian Weng](https://lilianweng.github.io)
  * [Controllable Neural Text Generation](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/) (Jan 2021)
  * [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/) (Mar 2023)
* K. Rink: [Leverage LLMs Like GPT to Analyze Your Documents or Transcripts](https://towardsdatascience.com/leverage-llms-like-gpt-to-analyze-your-documents-or-transcripts-c640a266ad52) (2023)
* Jay Alammar: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer) (2019)
* Sebastian Raschka:
  * [Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) (2024)
  * [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models) (2023). A Cross-Section of the Most Relevant Literature To Get Up to Speed.
  * [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) (2023)
* Databricks:
  * Mike Conover et al: [Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)
  * [Databricks-GPU-Serving-Examples](https://github.com/ahdbilal/Databricks-GPU-Serving-Examples)
* S. Vivek: [Build A Custom AI Based ChatBot Using Langchain, Weviate, and Streamlit](https://pub.towardsai.net/build-a-custom-ai-based-chatbot-using-langchain-weviate-and-streamlit-568efcdefca1) [github](https://github.com/LLM-Projects/docs-qa-bot/tree/main)
* Ravi Theja: [LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews](https://medium.com/llamaindex-blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b) (2023)
* [Eugene Yan](https://eugeneyan.com/):
  * [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/) (2023)
  * [Obsidian-Copilot: An Assistant for Writing & Reflecting](https://eugeneyan.com/writing/obsidian-copilot/) (2023)
  * [open-llms](https://github.com/eugeneyan/open-llms)
* LlamaIndex: Jerry Liu: [8 key considerations for building *production-grade* LLM apps over your data](https://twitter.com/jerryjliu0/status/1692931028963221929)
  * [LLamaIndex Webinar](https://www.youtube.com/watch?v=Zj5RCweUHIk) (2023)
  * [This weekend I was inspired to create “Recursive Document Agents”...](https://twitter.com/jerryjliu0/status/1693421308674289822?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email) (2023)
* [Ori Eldarov](https://substack.com/@fullydistributed): [McKinsey's Lilli: A Wake-Up Call for AI Startups](https://www.fullydistributed.co/p/mckinseys-lilli-a-wake-up-call-for) (2022)
* Rick Lamers:
  * [Never write a single shell command ever again! Today I'm releasing Shell AI...](https://twitter.com/RickLamers/status/1693365277793137067?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email)
* ArsTechnica: B Edwards: [10X coders beware: Meta’s new AI model boosts coding and debugging for free](https://arstechnica.com/information-technology/2023/08/meta-introduces-code-llama-an-ai-tool-aimed-at-faster-coding-and-debugging/) (2023)
* Cameron Wolfe
  * [Deep (Learning) Focus](https://cameronrwolfe.substack.com/) substack
    * [The Basics of AI-Powered (Vector) Search](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search) (2024)
    * [Google Gemini: Fact or Fiction?](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction) (2023)
    * [Language understanding with BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) (2022)
  * X: [Generative large language models (LLMs) are based upon the decoder-only transformer architecture. Currently, these types of generative LLMs are incredibly popular. However, I use encoder-only architectures for 90% of use cases as a practitioner. Here’s why…](https://twitter.com/cwolferesearch/status/1745876867003920517) (2023)
* Project Pro: [BERT NLP Model Explained for Complete Beginners](https://www.projectpro.io/article/bert-nlp-model-explained/558)
* F. Gichere: [Sentiment Analysis of App Reviews: A Comparison of BERT, spaCy, TextBlob, and NLTK](https://francisgichere.medium.com/sentiment-analysis-of-app-reviews-a-comparison-of-bert-spacy-textblob-and-nltk-9016054d54dc) (2023)
* B. Etienne: [A Complete Guide to Write your own Transformers](https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd) (2024)

#### Talks
* [Advanced Natural Language Processing with Apache Spark NLP](https://www.youtube.com/watch?v=V9NBkxvSZmU&t=237s) (2021)
* Yannic Kilcher
  * [GPT-3: Language Models are Few-Shot Learners](https://www.youtube.com/watch?v=SY5PvZrJhLE&list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu&index=20) (Paper Explained)
* PI School: Lukasz Kaiser: [Attention is all you need; Attention neural network models](https://www.youtube.com/watch?v=rBCqOTEfxvg) (2018)
* [John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges](https://www.youtube.com/watch?v=hhiLw5Q_UFg) (2023)
* [Mapping the future of *truly* Open Models and Training Dolly for $30 — with Mike Conover of Databricks](https://www.latent.space/p/mike-conover?utm_source=twitter&utm_medium=social&utm_campaign=twitter-embed#details) (2023)
* Latent Space:
  * [Latent Space Live: Responding to the Leaked Google vs OpenAI strategy memo ](https://www.youtube.com/watch?v=GR8i7PfWaVw) (2023)
  * [Training Mosaic's "llongboi" MPT-7B in 9 days for $200k with an empty logbook, how to prep good data for your training, and the future of open models](https://www.latent.space/p/mosaic-mpt-7b#details) (2023)
  * [The Mathematics of Training LLMs — with Quentin Anthony of Eleuther AI](https://www.latent.space/p/transformers-math)
* [James Briggs](https://www.youtube.com/c/jamesbriggs)
  * [Llama2: AI Developer Handbook](https://www.pinecone.io/learn/llama-2/) (2023)
    * 4 bit quantization reduces VRAM reqirement 8x
    * Llama2 70B model with 4 bit quantization requires 35GB VRAM. Use A100 (AWS p4d) which has 40GB VRAM.
    * Llama2 13B with 4 bit quantization requires requires 7GB VRAM. Use T4 which has 16G VRAM.
    * Llama2 7B requires 3.5GB VRAM.
  * Langchain, Llama2 quantized, Pinecone: [Better Llama 2 with Retrieval Augmented Generation (RAG)](https://www.youtube.com/watch?v=ypzmPwLH_Q4) (2023)
* [LangChain Talk](https://docs.google.com/presentation/d/1exjoapZ4EB_2xSQ7BSfSdPsZT30ap_6N3bi37F3cG0E/edit#slide=id.g23a07b78d21_0_217) (2023)
* Yann LeCun: [Objective-Driven AI](https://drive.google.com/file/d/1wzHohvoSgKGZvzOWqZybjm4M4veKR6t3/view) (2023)
* Ilya Sutskever: [An observation on Generalization](https://www.youtube.com/watch?v=AKMuA_TVz3A) (2023)
* Kamalraj M M:
  * [Loading PDF Data Into Langchain : To Use Or Not To Use Unstructured Library ](https://www.youtube.com/watch?v=Sbm1rGsZG2g) (2023)
  * [Langchain VectorStores Show Down : Which One Reigns Supreme?](https://www.youtube.com/watch?v=zGAkhN1YZXM) (2023)
* Aleksa Gordic: [Will LLMs kill Search? Nils Reimers (director of ML at Cohere](https://www.youtube.com/watch?v=bXdR46em2-M)(2023)
* Connor Shorten: [MemGPT Explained!](https://www.youtube.com/watch?v=nQmZmFERmrg) (2023), [paper](https://arxiv.org/pdf/2310.08560.pdf)
* Connor Shorten: [Charles Packer on MemGPT](https://www.youtube.com/watch?v=rxjsbUiuOFo) (2023)
* OpenAI: [A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y&t=276s) (2023)
* Qingyun Wu et al: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155) (2023)

#### Tools
* [Langchain](https://python.langchain.com/en/latest/getting_started/getting_started.html), see [Langchain](/software_stacks/langchain)
* [LLamaHub](https://llamahub.ai/), [github](https://github.com/emptycrown/llama-hub)
* [Chromadb](https://www.trychroma.com/), see [Chromadb](/software_stacks/chromadb)
* [Pinecone](https://www.pinecone.io/)
* [Replit](https://replit.com)
* [Langview](https://twitter.com/rupert_parry/status/1653780093712633859) demo
* [Mosaic](https://www.mosaicml.com) - open source LLM infrastructure for enterprise market.
  * [BioMedLM: a Domain-Specific Large Language Model for Biomedical Text](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (2022)
    * Data stored in S3
    * Training on cheaper cloud infrastructure than EC2
    * Data loading with [mosaicml/streaming](https://github.com/mosaicml/streaming)
 * OpenAI, see [OpenAI](/software_stacks/openai)
 * [Unstructured](https://www.unstructured.io/)
   * [How Unstructured Unlocked 100k+ Pages of IRS Manuals](https://medium.com/@unstructured-io/leveraging-enterprise-specific-data-with-llms-how-unstructured-unlocked-100k-pages-of-irs-manuals-33e16308c1e3) (2023)
* Summarization
  * [WordTune](http://wordtune.com)
* [llama2-chatbot](https://llama2.ai/) [github](https://github.com/a16z-infra/llama2-chatbot) by Matt Bornstein et al
* [SciSpace Copilot](https://typeset.io/) chrome extension
* [Ollama](https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image)
* Prompt Engineering
  * Langsmith
  * Humanloop
  * [Trubrics](https://trubrics.com/)
  * [Agenta](https://github.com/agenta-ai/agenta?tab=readme-ov-file), open source

#### Models
* [OpenAlpaca](https://github.com/yxuansu/OpenAlpaca)
* [alpaca-lora](https://github.com/tloen/alpaca-lora), [huggingface](https://huggingface.co/spaces/tloen/alpaca-lora)
* Vicuna [FastChat](https://github.com/lm-sys/FastChat)
  * [Chat with Open Large Language Models](https://chat.lmsys.org/)
  * [Run Vicuna-13B On Your Local Computer](https://www.youtube.com/watch?v=F_pFH-AngoE), tutorial (GPU)
  * Nischal Harohalli Padmanabha: [A step-by-step guide to running Vicuna-13B Large Language Model on your GPU / CPU machine](https://www.linkedin.com/pulse/step-by-step-guide-running-vicuna-13b-large-language-nischal/) (2023)
    * g4dn.4xlarge EC2 instance
      * Memory - 64GB
      * GPU - Not mandatory, but advised. Tesla T4 - 16GB
      * CPU - 16 Core
      * Disk space - 200 GB
* Simon Willison's Weblog:
  * [Large language models are having their Stable Diffusion moment](https://simonwillison.net/2023/Mar/11/llama/) (2023)
  * [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp](https://til.simonwillison.net/llms/llama-7b-m2) (2023)
* [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
  * ggerganov: [What is the meaning of hacked?](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022)
* [gorilla](https://github.com/ShishirPatil/gorilla)
  * Ben Wodecky: [Meet Gorilla: The AI Model That Beats GPT-4 at API Calls](https://aibusiness.com/nlp/meet-gorilla-the-ai-model-that-beats-gpt-4-at-api-calls) (2022)
* [toolformer](https://github.com/lucidrains/toolformer-pytorch)

#### Scale of Models
* Harm de Vries: [Go smol or go home](https://www.harmdevries.com/post/model-size-vs-compute-overhead/), Apr 2023
  * Andrej Karpathy tweet: [The 'Chincilla trap' and the deVries post](https://twitter.com/karpathy/status/1654898539661754368)
* [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)
* Tim Dettmers tweet: [Next week: bitsandbytes 4-bit closed beta that allows you to finetune 30B/65B LLaMA models on a single 24/48 GB GPU (no degradation vs full fine-tuning in 16-bit)](https://twitter.com/Tim_Dettmers/status/1654917326381228033), May 2023

#### Fine Tuning
* [huggingface/peft](https://github.com/huggingface/peft), supports LORA, Prefix-Tuning, P-Tuning, Prompt Tuning, AdaLoRA. Runs on consumer hardware.
  * Mark Tenenholtz: [tweet](https://twitter.com/marktenenholtz/status/1655582062663852036): Everyone can fine-tune LLMs on a single GPU.
* Databricks:
  * Sean Owen: [Fine-Tuning Large Language Models with Hugging Face and DeepSpeed](https://www.databricks.com/blog/2023/03/20/fine-tuning-large-language-models-hugging-face-and-deepspeed.html) (2023)
* Maxime Labonne: [Fine-Tune Your Own Llama 2 Model in a Colab Notebook](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) (2023)

#### RLHF, DPO
* HuggingFace: [Reinforcement Learning from Human Feedback: From Zero to chatGPT ](https://www.youtube.com/watch?v=2MBJOuVq380) (2023), [blog post](https://huggingface.co/blog/rlhf), [slides](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUMxOHpGR3JyS29iS1Z4YjF6X2ZfRDAxamNaQXxBQ3Jtc0ttZ3hWX1lFRFJjOFZrXzhOVmZRQUlhcXA5VnNpVGt1Zkw0SkVNTEhJRWg3WTBLZC15SGZ6NDdCdlZBX1ZrbEU4eVp4Zmp1U2N5M185NDZRLTNkWVRPOGVTV2dBeU9tREktMlpSUnRUWEpSVlRqSkNzbw&q=https%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1eI9PqRJTCFOIVihkig1voRM4MHDpLpCicX9lX1J2fqk%2Fedit%3Fusp%3Dsharing&v=2MBJOuVq380)
* HuggingFace: [Aligning LLMs with Direct Preference Optimization](https://www.youtube.com/watch?v=QXVCqtAZAn4) (2024)

#### Evaluation
* Arthur AI: [LLMs for Evaluating LLMs](https://www.youtube.com/watch?v=jW290vZThgw)(2024)

#### Open Source Movement
* Andrej Karpathy tweet: [Roughly speaking the story as of now](https://twitter.com/karpathy/status/1654892810590650376?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email), Apr 2023
  

#### Companies
* Hugging Face
  * AssemblyAI: [Getting Started With Hugging Face in 15 Minutes: Transformers, Pipeline, Tokenizer, Models](https://www.youtube.com/watch?v=QEaBAZQCtwE) (2022)
  * 1littecoder: [What is Hugging Face - Crash Course (No Coding): ML Products for Beginners](https://www.youtube.com/watch?v=x8gdOPO35HA) (2022)
* Llamaindes: see [LlamaIndex](/software_stacks/llamaindex)
* Weaviate: see [Weaviate Software Stack](/software_stacks/weaviate)

#### Medtech
* [ChatGPT: Will it transform the world of healthcare?](https://www.youtube.com/watch?v=j-aOCuzfxUI&t=779s)

#### Sales, Marketing apps
* [AI for Salespeople](https://nbold.co/ai-for-salespeople/)
* [Chrystalknows](https://Crystalknows.com) (Chrome extension)

#### Law
* [ICML Generative AI + Law (GenLaw)](https://genlaw.github.io/), [recordings](https://www.youtube.com/live/5j4U2UzJWfI) (2023)


#### Other
* [Artificial Intelligence](/artificial_intelligence)
* [AI Agents](/ai_agents)
* [Cognitive Science](/cognitive_science)
* [Computation Theory](/computation_theory)
* [Computational Topology](/computational_topology)
* [Document Classification](/document_classification)
* [Finance](/finance)
* [Language Models](/language_models)
* [Meta Learning](/meta_learning)
* [Probabilities and Statistics](/probabilities_and_statistics)
* [Robotics](/robotics)
* [Self Driving Cars](/self_driving_cars)
